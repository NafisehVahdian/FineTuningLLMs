# -*- coding: utf-8 -*-
"""finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EL-ocX3W1VYSG5hW72TMh2FFdlDO5J3Q
"""

!pip install -q "transformers>=4.40.0" "datasets" "peft" "accelerate" "bitsandbytes"

import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"
os.environ["WANDB_START_METHOD"] = "thread"
os.environ["WANDB_SILENT"] = "true"

from datasets import load_dataset

dataset = load_dataset("json", data_files="math_tutor_train_v2.jsonl")
train_dataset = dataset["train"]

print(train_dataset[0])
print("Number of examples:", len(train_dataset))

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,   # load model in 4-bit (saves GPU memory)
    device_map="auto"    # automatically use GPU
)

print("Model + tokenizer loaded.")

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,                   # rank of LoRA matrices (small = fast & light)
    lora_alpha=16,         # scaling factor
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM", # we are training a causal language model
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

def format_example(example):
    instruction = example["instruction"]
    output = example["output"]

    # Build a simple prompt format
    text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

    # Turn text into token IDs
    tokenized = tokenizer(
        text,
        truncation=True,
        max_length=256,
        padding="max_length"
    )

    # For causal language modelling, labels = input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_train = train_dataset.map(format_example)

print(tokenized_train[0].keys())
print("Tokenized training examples:", len(tokenized_train))

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir="./math-tutor-lora",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=1,
    save_steps=50,
    fp16=True,
    report_to="none",
)



data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False   # mlm = masked language modeling (not used for chat models)
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    data_collator=data_collator,
)

trainer.train()

model.save_pretrained("./math-tutor-lora")
tokenizer.save_pretrained("./math-tutor-lora")

print("Fine-tuned LoRA adapter and tokenizer saved.")

from transformers import pipeline
from peft import PeftModel

# Reload the base model in 4-bit
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto"
)

# Load your fine-tuned LoRA adapter
ft_model = PeftModel.from_pretrained(base_model, "./math-tutor-lora")

ft_model.eval()

# Build a text-generation pipeline
pipe = pipeline(
    "text-generation",
    model=ft_model,
    tokenizer=tokenizer,
    max_length=150,
    do_sample=True,
    top_p=0.9
)

def ask(question):
    prompt = f"### Instruction:\n{question}\n\n### Response:\n"
    answer = pipe(prompt)[0]["generated_text"]
    print(answer)
    print("\n" + "="*60 + "\n")

ask("What is 27 + 14?")
ask("Explain what speed means.")
ask("Which number is larger, 0.4 or 0.09?")